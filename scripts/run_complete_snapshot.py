#!/usr/bin/env python3
"""
Run complete pipeline snapshot demonstrating all Sprint 4 components + Enhanced Features.

Showcases:
- Sprint 1: Orthography (normalization, cleaning)
- Sprint 2: Evaluation (metrics, confusion matrices)
- Sprint 3: Morphology (feature extraction)
- Sprint 4: Syntax (I3rab parsing, prediction, evaluation)
- Enhanced: Particle detection using operators_catalog_split.csv
- Enhanced: CV pattern analysis (C=consonant, V=vowel)
- Enhanced: Tri-literal root extraction

Usage (from repo root):
  PYTHONPATH=src python3 scripts/run_complete_snapshot.py
  PYTHONPATH=src python3 scripts/run_complete_snapshot.py --output snapshot_out.json
  PYTHONPATH=src python3 scripts/run_complete_snapshot.py --verbose
"""

import argparse
import json
import sys
import csv
from pathlib import Path
from typing import Dict, List, Any, Optional

# Ø¢ÙŠØ© Ø§Ù„Ø¯ÙŠÙ† (Ø³ÙˆØ±Ø© Ø§Ù„Ø¨Ù‚Ø±Ø© 2:282)
AYAT_AL_DAYN = (
    "ÙŠÙØ§ Ø£ÙÙŠÙÙ‘Ù‡ÙØ§ Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ Ø¥ÙØ°ÙØ§ ØªÙØ¯ÙØ§ÙŠÙÙ†ØªÙÙ… Ø¨ÙØ¯ÙÙŠÙ’Ù†Ù Ø¥ÙÙ„ÙÙ‰ Ø£ÙØ¬ÙÙ„Ù Ù…ÙÙ‘Ø³ÙÙ…Ù‹Ù‘Ù‰ ÙÙØ§ÙƒÙ’ØªÙØ¨ÙÙˆÙ‡Ù ÙˆÙÙ„Ù’ÙŠÙÙƒÙ’ØªÙØ¨ Ø¨ÙÙ‘ÙŠÙ’Ù†ÙÙƒÙÙ…Ù’ ÙƒÙØ§ØªÙØ¨ÙŒ Ø¨ÙØ§Ù„Ù’Ø¹ÙØ¯Ù’Ù„Ù ÙˆÙÙ„ÙØ§ ÙŠÙØ£Ù’Ø¨Ù ÙƒÙØ§ØªÙØ¨ÙŒ Ø£ÙÙ† ÙŠÙÙƒÙ’ØªÙØ¨Ù ÙƒÙÙ…ÙØ§ Ø¹ÙÙ„ÙÙ‘Ù…ÙÙ‡Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙÙÙ„Ù’ÙŠÙÙƒÙ’ØªÙØ¨Ù’ ÙˆÙÙ„Ù’ÙŠÙÙ…Ù’Ù„ÙÙ„Ù Ø§Ù„ÙÙ‘Ø°ÙÙŠ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù Ø§Ù„Ù’Ø­ÙÙ‚ÙÙ‘ ÙˆÙÙ„Ù’ÙŠÙØªÙÙ‘Ù‚Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘Ù‡Ù ÙˆÙÙ„ÙØ§ ÙŠÙØ¨Ù’Ø®ÙØ³Ù’ Ù…ÙÙ†Ù’Ù‡Ù Ø´ÙÙŠÙ’Ø¦Ø§Ù‹ ÙÙØ¥ÙÙ† ÙƒÙØ§Ù†Ù Ø§Ù„ÙÙ‘Ø°ÙÙŠ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù Ø§Ù„Ù’Ø­ÙÙ‚ÙÙ‘ Ø³ÙÙÙÙŠÙ‡Ø§Ù‹ Ø£ÙÙˆÙ’ Ø¶ÙØ¹ÙÙŠÙØ§Ù‹ Ø£ÙÙˆÙ’ Ù„ÙØ§ ÙŠÙØ³Ù’ØªÙØ·ÙÙŠØ¹Ù Ø£ÙÙ† ÙŠÙÙ…ÙÙ„ÙÙ‘ Ù‡ÙÙˆÙ ÙÙÙ„Ù’ÙŠÙÙ…Ù’Ù„ÙÙ„Ù’ ÙˆÙÙ„ÙÙŠÙÙ‘Ù‡Ù Ø¨ÙØ§Ù„Ù’Ø¹ÙØ¯Ù’Ù„Ù ÙˆÙØ§Ø³Ù’ØªÙØ´Ù’Ù‡ÙØ¯ÙÙˆØ§ Ø´ÙÙ‡ÙÙŠØ¯ÙÙŠÙ’Ù†Ù Ù…ÙÙ† Ø±ÙÙ‘Ø¬ÙØ§Ù„ÙÙƒÙÙ…Ù’ ÙÙØ¥ÙÙ† Ù„ÙÙ‘Ù…Ù’ ÙŠÙÙƒÙÙˆÙ†ÙØ§ Ø±ÙØ¬ÙÙ„ÙÙŠÙ’Ù†Ù ÙÙØ±ÙØ¬ÙÙ„ÙŒ ÙˆÙØ§Ù…Ù’Ø±ÙØ£ÙØªÙØ§Ù†Ù Ù…ÙÙ…ÙÙ‘Ù† ØªÙØ±Ù’Ø¶ÙÙˆÙ’Ù†Ù Ù…ÙÙ†Ù Ø§Ù„Ø´ÙÙ‘Ù‡ÙØ¯ÙØ§Ø¡Ù Ø£ÙÙ† ØªÙØ¶ÙÙ„ÙÙ‘ Ø¥ÙØ­Ù’Ø¯ÙØ§Ù‡ÙÙ…ÙØ§ ÙÙØªÙØ°ÙÙƒÙÙ‘Ø±Ù Ø¥ÙØ­Ù’Ø¯ÙØ§Ù‡ÙÙ…ÙØ§ Ø§Ù„Ù’Ø£ÙØ®Ù’Ø±ÙÙ‰ ÙˆÙÙ„ÙØ§ ÙŠÙØ£Ù’Ø¨Ù Ø§Ù„Ø´ÙÙ‘Ù‡ÙØ¯ÙØ§Ø¡Ù Ø¥ÙØ°ÙØ§ Ù…ÙØ§ Ø¯ÙØ¹ÙÙˆØ§ ÙˆÙÙ„ÙØ§ ØªÙØ³Ù’Ø£ÙÙ…ÙÙˆØ§ Ø£ÙÙ† ØªÙÙƒÙ’ØªÙØ¨ÙÙˆÙ‡Ù ØµÙØºÙÙŠØ±Ø§Ù‹ Ø£ÙÙˆÙ’ ÙƒÙØ¨ÙÙŠØ±Ø§Ù‹ Ø¥ÙÙ„ÙÙ‰ Ø£ÙØ¬ÙÙ„ÙÙ‡Ù Ø°ÙÙ„ÙÙƒÙÙ…Ù’ Ø£ÙÙ‚Ù’Ø³ÙØ·Ù Ø¹ÙÙ†Ø¯Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙØ£ÙÙ‚Ù’ÙˆÙÙ…Ù Ù„ÙÙ„Ø´ÙÙ‘Ù‡ÙØ§Ø¯ÙØ©Ù ÙˆÙØ£ÙØ¯Ù’Ù†ÙÙ‰ Ø£ÙÙ„ÙÙ‘Ø§ ØªÙØ±Ù’ØªÙØ§Ø¨ÙÙˆØ§ Ø¥ÙÙ„ÙÙ‘Ø§ Ø£ÙÙ† ØªÙÙƒÙÙˆÙ†Ù ØªÙØ¬ÙØ§Ø±ÙØ©Ù‹ Ø­ÙØ§Ø¶ÙØ±ÙØ©Ù‹ ØªÙØ¯ÙÙŠØ±ÙÙˆÙ†ÙÙ‡ÙØ§ Ø¨ÙÙŠÙ’Ù†ÙÙƒÙÙ…Ù’ ÙÙÙ„ÙÙŠÙ’Ø³Ù Ø¹ÙÙ„ÙÙŠÙ’ÙƒÙÙ…Ù’ Ø¬ÙÙ†ÙØ§Ø­ÙŒ Ø£ÙÙ„ÙÙ‘Ø§ ØªÙÙƒÙ’ØªÙØ¨ÙÙˆÙ‡ÙØ§ ÙˆÙØ£ÙØ´Ù’Ù‡ÙØ¯ÙÙˆØ§ Ø¥ÙØ°ÙØ§ ØªÙØ¨ÙØ§ÙŠÙØ¹Ù’ØªÙÙ…Ù’ ÙˆÙÙ„ÙØ§ ÙŠÙØ¶ÙØ§Ø±ÙÙ‘ ÙƒÙØ§ØªÙØ¨ÙŒ ÙˆÙÙ„ÙØ§ Ø´ÙÙ‡ÙÙŠØ¯ÙŒ ÙˆÙØ¥ÙÙ† ØªÙÙÙ’Ø¹ÙÙ„ÙÙˆØ§ ÙÙØ¥ÙÙ†ÙÙ‘Ù‡Ù ÙÙØ³ÙÙˆÙ‚ÙŒ Ø¨ÙÙƒÙÙ…Ù’ ÙˆÙØ§ØªÙÙ‘Ù‚ÙÙˆØ§ Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙÙŠÙØ¹ÙÙ„ÙÙ‘Ù…ÙÙƒÙÙ…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙØ§Ù„Ù„ÙÙ‘Ù‡Ù Ø¨ÙÙƒÙÙ„ÙÙ‘ Ø´ÙÙŠÙ’Ø¡Ù Ø¹ÙÙ„ÙÙŠÙ…ÙŒ"
)


def load_operators_catalog(verbose: bool = False) -> Dict[str, Dict[str, Any]]:
    """Load operators catalog from CSV file."""
    catalog_path = Path("data/operators_catalog_split.csv")
    
    if not catalog_path.exists():
        if verbose:
            print(f"Warning: Operators catalog not found at {catalog_path}", file=sys.stderr)
        return {}
    
    operators = {}
    
    try:
        with open(catalog_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                operator = row['Operator'].strip()
                operators[operator] = {
                    "group_number": row['Group Number'],
                    "arabic_group": row['Arabic Group Name'],
                    "english_group": row['English Group Name'],
                    "purpose": row['Purpose/Usage'],
                    "example": row['Example'],
                    "note": row['Note'],
                }
        
        if verbose:
            print(f"Loaded {len(operators)} operators from catalog", file=sys.stderr)
        
    except Exception as e:
        if verbose:
            print(f"Error loading operators catalog: {e}", file=sys.stderr)
        return {}
    
    return operators


def load_mabniyat_catalog(verbose: bool = False) -> Dict[str, Dict[str, Any]]:
    """Load Mabniyat (indeclinable nouns/particles) from data/arabic_json/2."""
    catalog_path = Path("data/arabic_json/2")
    mabniyat = {}
    
    if not catalog_path.exists():
        if verbose:
            print(f"Warning: Mabniyat catalog path not found at {catalog_path}", file=sys.stderr)
        return {}
    
    count = 0
    try:
        # Recursive glob for all json files in data/arabic_json/2
        for json_file in catalog_path.rglob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if isinstance(data, list):
                    items = data
                else:
                    items = [data]
                    
                for item in items:
                    if not isinstance(item, dict):
                        continue
                        
                    # Key might be "Ø§Ù„Ø£Ø¯Ø§Ø©" or similar. We look for the main term.
                    # Based on inspected files, "Ø§Ù„Ø£Ø¯Ø§Ø©" is the common key.
                    word = item.get("Ø§Ù„Ø£Ø¯Ø§Ø©")
                    if not word:
                        continue
                        
                    # Clean the word for lookup key
                    clean_word = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')
                    
                    # Handle multiple forms like "Ø§Ù„Ù„Ù‘ÙØ°ÙØ§Ù†Ù / Ø§Ù„Ù„Ù‘ÙØ°ÙÙŠÙ’Ù†Ù"
                    forms = [f.strip() for f in clean_word.split('/')]
                    
                    for form in forms:
                        if form:
                            mabniyat[form] = item
                            count += 1
                            
            except Exception as e:
                if verbose:
                    print(f"Error loading {json_file}: {e}", file=sys.stderr)
                    
        if verbose:
            print(f"Loaded {count} Mabniyat entries from catalog", file=sys.stderr)
            
    except Exception as e:
        if verbose:
            print(f"Error walking mabniyat catalog: {e}", file=sys.stderr)
            
    return mabniyat


def detect_cv_pattern(word: str) -> Dict[str, Any]:
    """Detect CV (Consonant-Vowel) pattern in Arabic word."""
    # Arabic consonants
    consonants = set("Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙŠ")
    
    # Arabic short vowels (diacritics)
    short_vowels = {
        'Ù': 'a',  # fatha
        'Ù': 'u',  # damma
        'Ù': 'i',  # kasra
    }
    
    # Arabic long vowels
    long_vowels = {
        'Ø§': 'aa',
        'Ùˆ': 'uu',
        'ÙŠ': 'ii',
        'Ù‰': 'aa',
    }
    
    pattern = []
    phonetic = []
    i = 0
    
    while i < len(word):
        char = word[i]
        
        # Skip non-alphabetic diacritics
        if char in 'Ù‹ÙŒÙÙ‘Ù’Ù°':
            i += 1
            continue
        
        # Consonant
        if char in consonants:
            pattern.append('C')
            phonetic.append(char)
            
            # Check for following vowel diacritic
            if i + 1 < len(word) and word[i + 1] in short_vowels:
                pattern.append('V')
                phonetic.append(short_vowels[word[i + 1]])
                i += 2
            else:
                i += 1
        
        # Long vowel (treated as V)
        elif char in long_vowels:
            pattern.append('V')
            phonetic.append(long_vowels[char])
            i += 1
        
        # Hamza and alef variants
        elif char in 'Ø¡Ø£Ø¥Ø¢Ø¤Ø¦':
            pattern.append('C')
            phonetic.append('Ê”')
            i += 1
        
        else:
            i += 1
    
    pattern_str = ''.join(pattern)
    phonetic_str = ''.join(phonetic)
    
    # Classify common patterns
    pattern_type = None
    if pattern_str == 'CVCVC':
        pattern_type = 'faÊ•al (ÙÙØ¹ÙÙ„)'
    elif pattern_str == 'CVCCVC':
        pattern_type = 'faÊ•Ê•al (ÙÙØ¹ÙÙ‘Ù„)'
    elif pattern_str == 'CVCVVC':
        pattern_type = 'faÊ•aal (ÙÙØ¹ÙØ§Ù„)'
    elif pattern_str == 'CVCVVCVC':
        pattern_type = 'faÊ•aalah (ÙÙØ¹ÙØ§ï¿½ï¿½ÙØ©)'
    elif pattern_str == 'CVCCVVC':
        pattern_type = 'mafÊ•uul (Ù…ÙÙÙ’Ø¹ÙÙˆÙ„)'
    elif pattern_str == 'CVCVCCVC':
        pattern_type = 'mufaÊ•Ê•il (Ù…ÙÙÙØ¹ÙÙ‘Ù„)'
    
    return {
        "pattern": pattern_str,
        "phonetic": phonetic_str,
        "pattern_type": pattern_type,
        "length": len(pattern),
        "consonant_count": pattern.count('C'),
        "vowel_count": pattern.count('V'),
    }


def extract_root(word: str, mabniyat_catalog: Optional[Dict[str, Dict[str, Any]]] = None) -> Dict[str, Any]:
    """Extract tri-literal root from Arabic word."""
    # Remove diacritics
    clean = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')
    
    original_clean = clean
    
    # Check Mabniyat Catalog first
    if mabniyat_catalog and clean in mabniyat_catalog:
        mabniyat_info = mabniyat_catalog[clean]
        return {
            "original_word": word,
            "cleaned": original_clean,
            "stem": clean,
            "root_trilateral": None,
            "root_quadrilateral": None,
            "root_type": "mabni",  # Indeclinable
            "confidence": 1.0,
            "consonants_extracted": 0,
            "method": "knowledge_base_lookup",
            "mabniyat_info": {
                "type": mabniyat_info.get("Ø§Ù„Ù†ÙˆØ¹"),
                "grammatical_case": mabniyat_info.get("Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø­ÙˆÙŠØ©"),
                "number": mabniyat_info.get("Ø§Ù„Ø¹Ø¯Ø¯"),
                "gender": mabniyat_info.get("Ø§Ù„Ø¬Ù†Ø³") or mabniyat_info.get("Ø§Ù„Ø¬Ù†Ø³ "),
            }
        }
    
    # Remove common prefixes (longest first)
    prefixes = ["Ø§Ù„", "ÙˆØ§Ù„", "ÙØ§Ù„", "Ø¨Ø§Ù„", "ÙƒØ§Ù„", "Ù„Ù„", "Ùˆ", "Ù", "Ø¨", "Ù„", "Ùƒ", "Ø³", "Øª", "ÙŠ", "Ù†", "Ø£"]
    for prefix in prefixes:
        if clean.startswith(prefix) and len(clean) > len(prefix) + 2:
            clean = clean[len(prefix):]
            break
    
    # Remove common suffixes (longest first)
    suffixes = ["ÙˆÙ†Ù‡", "ÙˆÙ‡Ø§", "Ù‡Ù…Ø§", "ÙƒÙ…Ø§", "ÙƒÙ†", "Ù‡Ù…", "Ù‡Ù†", "Ù†Ø§", "Ù†ÙŠ", "ÙˆØ§", "ÙˆÙ†", "ÙŠÙ†", "Ø§Ù†", "ØªØ§Ù†", "ØªÙŠÙ†", "Ø©", "Ù‡", "Ù‡Ø§", "Øª", "Ùƒ", "ÙŠ"]
    for suffix in suffixes:
        if clean.endswith(suffix) and len(clean) > len(suffix) + 2:
            clean = clean[:-len(suffix)]
            break
    
    # Extract consonantal root
    consonants = []
    weak_letters = set("Ø§ÙˆÙ‰ÙŠØ¡Ø¢Ø£Ø¥Ø¤Ø¦")
    
    for char in clean:
        if char.isalpha() and char not in "Ù€":
            # Skip long vowels in the middle, but keep if at start
            if len(consonants) > 0 and char in weak_letters:
                continue
            consonants.append(char)
    
    # Take first 3-4 consonants as root
    if len(consonants) >= 3:
        root_3 = ''.join(consonants[:3])
        root_4 = ''.join(consonants[:4]) if len(consonants) >= 4 else None
        confidence = 0.7 if len(consonants) == 3 else 0.6
    elif len(consonants) == 2:
        root_3 = ''.join(consonants)
        root_4 = None
        confidence = 0.3
    else:
        root_3 = None
        root_4 = None
        confidence = 0.0
    
    return {
        "original_word": word,
        "cleaned": original_clean,
        "stem": clean,
        "root_trilateral": root_3,
        "root_quadrilateral": root_4,
        "root_type": "trilateral" if root_3 and len(root_3) == 3 else "quadrilateral" if root_4 else "unknown",
        "confidence": confidence,
        "consonants_extracted": len(consonants),
        "method": "morphological_stripping",
    }


def detect_operator(word: str, operators_catalog: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
    """Detect Arabic operator (particle/verb) using catalog."""
    # Remove diacritics for matching
    clean_word = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')
    
    # Direct match
    if clean_word in operators_catalog:
        return {
            "is_operator": True,
            "operator": clean_word,
            "original_word": word,
            **operators_catalog[clean_word],
        }
    
    # Check for prefixed operators
    prefixes = ["Ùˆ", "Ù", "Ø¨", "Ù„", "Ùƒ"]
    for prefix in prefixes:
        if clean_word.startswith(prefix) and len(clean_word) > 1:
            stem = clean_word[1:]
            if stem in operators_catalog:
                return {
                    "is_operator": False,
                    "has_operator_prefix": True,
                    "prefix": prefix,
                    "prefix_operator": operators_catalog.get(prefix, {}),
                    "stem": stem,
                    "stem_operator": operators_catalog.get(stem, {}),
                    "original_word": word,
                }
    
    return {
        "is_operator": False,
        "has_operator_prefix": False,
        "original_word": word,
    }


def demo_i3rab_parser(verbose: bool = False) -> Dict[str, Any]:
    """Demonstrate I3rab Parser (Task 4.2)."""
    try:
        from fvafk.c2b.syntax import I3rabParser
    except ImportError:
        return {"error": "I3rab Parser not available"}
    
    if verbose:
        print("\n=== SPRINT 4 - TASK 4.2: I3RAB PARSER ===", file=sys.stderr)
    
    parser = I3rabParser()
    
    examples = [
        {
            "word": "Ø§Ù„Ù’Ø­ÙÙ…Ù’Ø¯Ù",
            "i3rab": "Ù…Ø¨ØªØ¯Ø£ Ù…Ø±ÙÙˆØ¹ ÙˆØ¹Ù„Ø§Ù…Ø© Ø±ÙØ¹Ù‡ Ø§Ù„Ø¶Ù…Ø© Ø§Ù„Ø¸Ø§Ù‡Ø±Ø© Ø¹Ù„Ù‰ Ø¢Ø®Ø±Ù‡",
            "expected_type": "mubtada",
            "expected_case": "nominative"
        },
        {
            "word": "Ù„ÙÙ„ÙÙ‘Ù‡Ù",
            "i3rab": "Ø§Ø³Ù… Ù…Ø¬Ø±ÙˆØ± ÙˆØ¹Ù„Ø§Ù…Ø© Ø¬Ø±Ù‡ Ø§Ù„ÙƒØ³Ø±Ø© Ø§Ù„Ø¸Ø§Ù‡Ø±Ø©",
            "expected_type": "mudaf_ilayhi",
            "expected_case": "genitive"
        },
        {
            "word": "Ø±ÙØ¨ÙÙ‘",
            "i3rab": "Ù…Ø¶Ø§Ù Ø¥Ù„ÙŠÙ‡ Ù…Ø¬Ø±ÙˆØ± ÙˆØ¹Ù„Ø§Ù…Ø© Ø¬Ø±Ù‡ Ø§Ù„ÙƒØ³Ø±Ø©",
            "expected_type": "mudaf_ilayhi",
            "expected_case": "genitive"
        },
        {
            "word": "ÙƒÙØªÙØ§Ø¨Ø§Ù‹",
            "i3rab": "Ù…ÙØ¹ÙˆÙ„ Ø¨Ù‡ Ù…Ù†ØµÙˆØ¨ ÙˆØ¹Ù„Ø§Ù…Ø© Ù†ØµØ¨Ù‡ Ø§Ù„ÙØªØ­Ø© Ø§Ù„Ø¸Ø§Ù‡Ø±Ø©",
            "expected_type": "maf'ul_bihi",
            "expected_case": "accusative"
        },
        {
            "word": "ÙÙÙŠ",
            "i3rab": "Ø­Ø±Ù Ø¬Ø± Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ³Ø± Ù„Ø§ Ù…Ø­Ù„ Ù„Ù‡ Ù…Ù† Ø§Ù„Ø¥Ø¹Ø±Ø§Ø¨",
            "expected_type": "harf",
            "expected_case": None
        },
    ]
    
    parsed_examples = []
    for ex in examples:
        result = parser.parse(ex["i3rab"])
        
        parsed_examples.append({
            "word": ex["word"],
            "i3rab_text": ex["i3rab"],
            "parsed": {
                "i3rab_type": result.i3rab_type,
                "case": result.case,
                "case_marker": result.case_marker,
                "mahall": result.mahall,
                "confidence": result.confidence,
            },
            "expected": {
                "i3rab_type": ex["expected_type"],
                "case": ex["expected_case"],
            },
            "match": result.i3rab_type == ex["expected_type"]
        })
        
        if verbose:
            status = "âœ…" if parsed_examples[-1]["match"] else "âŒ"
            print(f"{status} {ex['word']}: {result.i3rab_type} (conf: {result.confidence:.2f})", file=sys.stderr)
    
    accuracy = sum(1 for p in parsed_examples if p["match"]) / len(parsed_examples)
    
    if verbose:
        print(f"Parser Accuracy: {accuracy:.1%}", file=sys.stderr)
    
    return {
        "component": "I3rab Parser",
        "task": "4.2",
        "examples_parsed": len(parsed_examples),
        "accuracy": accuracy,
        "examples": parsed_examples,
    }


def demo_syntax_evaluator(verbose: bool = False) -> Dict[str, Any]:
    """Demonstrate Syntax Evaluator (Task 4.3)."""
    try:
        from fvafk.c2b.syntax import SyntaxEvaluator, I3rabComponents
    except ImportError:
        return {"error": "Syntax Evaluator not available"}
    
    if verbose:
        print("\n=== SPRINT 4 - TASK 4.3: SYNTAX EVALUATOR ===", file=sys.stderr)
    
    predictions = [
        I3rabComponents(i3rab_type="mubtada", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="khabar", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="fa'il", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="maf'ul_bihi", case="accusative", case_marker="fatha"),
        I3rabComponents(i3rab_type="harf", case="genitive", case_marker="kasra"),
        I3rabComponents(i3rab_type="mudaf_ilayhi", case="genitive", case_marker="kasra"),
    ]
    
    gold = [
        I3rabComponents(i3rab_type="mubtada", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="khabar", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="fa'il", case="nominative", case_marker="damma"),
        I3rabComponents(i3rab_type="maf'ul_bihi", case="accusative", case_marker="fatha"),
        I3rabComponents(i3rab_type="harf", case="genitive", case_marker="kasra"),
        I3rabComponents(i3rab_type="mudaf_ilayhi", case="genitive", case_marker="kasra"),
    ]
    
    evaluator = SyntaxEvaluator()
    result = evaluator.evaluate(predictions, gold)
    
    if verbose:
        print(f"Overall Accuracy: {result.overall_accuracy():.1%}", file=sys.stderr)
        print(f"Overall F1: {result.overall_f1():.1%}", file=sys.stderr)
        print(f"Coverage: {result.coverage:.1%}", file=sys.stderr)
    
    summary = result.summary()
    i3rab_metrics = result.i3rab_type_metrics.to_dict()
    case_metrics = result.case_metrics.to_dict()
    marker_metrics = result.case_marker_metrics.to_dict()
    
    return {
        "component": "Syntax Evaluator",
        "task": "4.3",
        "words_evaluated": result.words_evaluated,
        "total_words": result.total_words,
        "overall_accuracy": result.overall_accuracy(),
        "overall_f1": result.overall_f1(),
        "coverage": result.coverage,
        "per_feature": {
            "i3rab_type": i3rab_metrics,
            "case": case_metrics,
            "case_marker": marker_metrics,
        },
        "detailed_summary": summary,
    }


def demo_morph_syntax_bridge(verbose: bool = False) -> Dict[str, Any]:
    """Demonstrate Morph-Syntax Bridge (Task 4.4)."""
    try:
        from fvafk.c2b.syntax import MorphSyntaxBridge
        from fvafk.c2b.morphology_flags import MorphologyFlags
    except ImportError:
        return {"error": "Morph-Syntax Bridge not available"}
    
    if verbose:
        print("\n=== SPRINT 4 - TASK 4.4: MORPH-SYNTAX BRIDGE ===", file=sys.stderr)
    
    bridge = MorphSyntaxBridge()
    
    sentence = [
        ("Ø§Ù„Ù’Ø­ÙÙ…Ù’Ø¯Ù", MorphologyFlags(case="nominative", definiteness=True)),
        ("Ù„ÙÙ„ÙÙ‘Ù‡Ù", MorphologyFlags(case="genitive", definiteness=False)),
        ("Ø±ÙØ¨ÙÙ‘", MorphologyFlags(case="genitive", definiteness=False)),
        ("Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ†Ù", MorphologyFlags(case="genitive", definiteness=True)),
    ]
    
    morphologies = [m for _, m in sentence]
    predictions = bridge.predict_sentence(morphologies)
    
    results = []
    for (word, morph), syntax in zip(sentence, predictions):
        results.append({
            "word": word,
            "morphology": {
                "case": morph.case,
                "definiteness": morph.definiteness,
            },
            "syntax": {
                "i3rab_type_ar": syntax.i3rab_type_ar,
                "i3rab_type_en": syntax.i3rab_type_en,
                "syntactic_role": syntax.syntactic_role,
                "case": syntax.case,
                "confidence": syntax.confidence,
            }
        })
        
        if verbose:
            print(f"{word}: {syntax.i3rab_type_en} ({syntax.syntactic_role}) - {syntax.confidence:.2f}", file=sys.stderr)
    
    return {
        "component": "Morph-Syntax Bridge",
        "task": "4.4",
        "sentence": "Ø§Ù„Ù’Ø­ÙÙ…Ù’Ø¯Ù Ù„ÙÙ„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘ Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ†Ù",
        "reference": "Al-Fatiha 1:2",
        "words_predicted": len(results),
        "rules_applied": 5,
        "predictions": results,
    }


def process_orthography(text: str, verbose: bool = False) -> Dict[str, Any]:
    """Process text through orthography module (Sprint 1)."""
    try:
        from fvafk.c2b.orthography import (
            normalize_arabic,
            clean_arabic_text,
            remove_diacritics,
            remove_tatweel,
        )
        
        if verbose:
            print("\n=== SPRINT 1: ORTHOGRAPHY ===", file=sys.stderr)
            print(f"Processing {len(text)} characters...", file=sys.stderr)
        
        return {
            "sprint": 1,
            "component": "Orthography",
            "original": text,
            "normalized": normalize_arabic(text),
            "cleaned": clean_arabic_text(text),
            "no_diacritics": remove_diacritics(text),
            "no_tatweel": remove_tatweel(text),
            "original_length": len(text),
            "cleaned_length": len(remove_diacritics(text)),
        }
        
    except (ImportError, AttributeError) as e:
        if verbose:
            print(f"Warning: Orthography module not fully available: {e}", file=sys.stderr)
        
        return {
            "sprint": 1,
            "component": "Orthography",
            "original": text,
            "normalized": text,
            "cleaned": text,
            "no_diacritics": text,
            "no_tatweel": text,
            "error": str(e),
        }


def extract_morphology(words: List[str], verbose: bool = False) -> List[Dict[str, Any]]:
    """Extract morphology features for words (Sprint 3)."""
    try:
        from fvafk.c2b.morphology_flags import MorphologyFlags, MorphologyFlagsExtractor
    except ImportError:
        if verbose:
            print("Warning: Morphology module not available", file=sys.stderr)
        return [{"word": w, "morphology": None} for w in words]
    
    if verbose:
        print(f"\n=== SPRINT 3: MORPHOLOGY ===", file=sys.stderr)
        print(f"Extracting features for {len(words)} words...", file=sys.stderr)
    
    extractor = MorphologyFlagsExtractor()
    
    results = []
    cases_found = {"nominative": 0, "accusative": 0, "genitive": 0, "none": 0}
    
    for word in words:
        morph = extractor.extract(word)
        
        if morph.case:
            cases_found[morph.case] += 1
        else:
            cases_found["none"] += 1
        
        morph_dict = {
            "case": morph.case,
            "number": morph.number,
            "gender": morph.gender,
            "definiteness": morph.definiteness,
            "confidence": morph.confidence,
        }
        
        results.append({
            "word": word,
            "morphology": morph_dict,
        })
    
    if verbose:
        print(f"Case distribution: {cases_found}", file=sys.stderr)
    
    return results


def predict_syntax(morphology_data: List[Dict[str, Any]], verbose: bool = False) -> List[Dict[str, Any]]:
    """Predict syntax from morphology (Sprint 4)."""
    try:
        from fvafk.c2b.syntax import MorphSyntaxBridge
        from fvafk.c2b.morphology_flags import MorphologyFlags
    except ImportError:
        if verbose:
            print("Warning: Syntax module not available", file=sys.stderr)
        return [{"word": d["word"], "morphology": d.get("morphology"), "syntax": None} for d in morphology_data]
    
    if verbose:
        print(f"\n=== SPRINT 4: SYNTAX PREDICTION ===", file=sys.stderr)
        print(f"Predicting syntax for {len(morphology_data)} words...", file=sys.stderr)
    
    morphologies = []
    for data in morphology_data:
        morph_dict = data.get("morphology", {})
        if morph_dict:
            morph = MorphologyFlags(
                case=morph_dict.get("case"),
                number=morph_dict.get("number"),
                gender=morph_dict.get("gender"),
                definiteness=morph_dict.get("definiteness"),
            )
        else:
            morph = MorphologyFlags()
        morphologies.append(morph)
    
    bridge = MorphSyntaxBridge()
    predictions = bridge.predict_sentence(morphologies)
    
    i3rab_dist = {}
    for pred in predictions:
        i3rab_type = pred.i3rab_type_en
        i3rab_dist[i3rab_type] = i3rab_dist.get(i3rab_type, 0) + 1
    
    if verbose:
        print(f"I3rab distribution: {i3rab_dist}", file=sys.stderr)
    
    results = []
    for data, syntax in zip(morphology_data, predictions):
        results.append({
            "word": data["word"],
            "morphology": data["morphology"],
            "syntax": {
                "i3rab_type_ar": syntax.i3rab_type_ar,
                "i3rab_type_en": syntax.i3rab_type_en,
                "syntactic_role": syntax.syntactic_role,
                "case": syntax.case,
                "confidence": syntax.confidence,
            }
        })
    
    return results


def enhance_word_analysis(words: List[str], operators_catalog: Dict[str, Dict[str, Any]], mabniyat_catalog: Dict[str, Dict[str, Any]], verbose: bool = False) -> List[Dict[str, Any]]:
    """Enhanced analysis: operators, CV patterns, roots."""
    if verbose:
        print(f"\n=== ENHANCED ANALYSIS: OPERATORS, CV PATTERNS, ROOTS ===", file=sys.stderr)
    
    results = []
    operator_count = 0
    root_count = 0
    mabni_count = 0
    pattern_types = {}
    
    for word in words:
        # Detect operator
        operator_info = detect_operator(word, operators_catalog)
        if operator_info["is_operator"]:
            operator_count += 1
        
        # CV pattern
        cv_pattern = detect_cv_pattern(word)
        if cv_pattern["pattern_type"]:
            pattern_types[cv_pattern["pattern_type"]] = pattern_types.get(cv_pattern["pattern_type"], 0) + 1
        
        # Root extraction
        root_info = extract_root(word, mabniyat_catalog)
        if root_info["root_trilateral"]:
            root_count += 1
        if root_info.get("root_type") == "mabni":
            mabni_count += 1
        
        results.append({
            "word": word,
            "operator_analysis": operator_info,
            "cv_pattern": cv_pattern,
            "root_extraction": root_info,
        })
    
    if verbose:
        print(f"Operators detected: {operator_count}/{len(words)} ({operator_count/len(words)*100:.1f}%)", file=sys.stderr)
        print(f"Roots extracted: {root_count}/{len(words)} ({root_count/len(words)*100:.1f}%)", file=sys.stderr)
        print(f"Mabniyat identified: {mabni_count}/{len(words)} ({mabni_count/len(words)*100:.1f}%)", file=sys.stderr)
        print(f"Pattern types found: {len(pattern_types)}", file=sys.stderr)
    
    return results


def compute_statistics(results: List[Dict[str, Any]], enhanced_analysis: List[Dict[str, Any]], verbose: bool = False) -> Dict[str, Any]:
    """Compute evaluation statistics (Sprint 2) + enhanced stats."""
    if verbose:
        print(f"\n=== SPRINT 2: EVALUATION & STATISTICS ===", file=sys.stderr)
    
    total_words = len(results)
    
    # Syntax stats
    syntax_predictions = sum(1 for r in results if r.get("syntax") and r["syntax"].get("i3rab_type_en") != "unknown")
    
    i3rab_counts = {}
    role_counts = {}
    for r in results:
        if r.get("syntax"):
            i3rab_type = r["syntax"].get("i3rab_type_en", "unknown")
            i3rab_counts[i3rab_type] = i3rab_counts.get(i3rab_type, 0) + 1
            
            role = r["syntax"].get("syntactic_role", "unknown")
            role_counts[role] = role_counts.get(role, 0) + 1
    
    confidences = [r["syntax"]["confidence"] for r in results if r.get("syntax")]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0
    
    # Morphology stats
    morph_with_case = sum(1 for r in results if r.get("morphology") and r["morphology"].get("case"))
    
    # Enhanced stats
    operator_stats = {
        "total_operators": sum(1 for e in enhanced_analysis if e["operator_analysis"]["is_operator"]),
        "operator_groups": {},
    }
    
    for e in enhanced_analysis:
        if e["operator_analysis"]["is_operator"]:
            group = e["operator_analysis"].get("english_group", "unknown")
            operator_stats["operator_groups"][group] = operator_stats["operator_groups"].get(group, 0) + 1
    
    root_stats = {
        "total_roots_extracted": sum(1 for e in enhanced_analysis if e["root_extraction"]["root_trilateral"]),
        "trilateral_roots": sum(1 for e in enhanced_analysis if e["root_extraction"]["root_type"] == "trilateral"),
        "quadrilateral_roots": sum(1 for e in enhanced_analysis if e["root_extraction"]["root_type"] == "quadrilateral"),
        "mabniyat_identified": sum(1 for e in enhanced_analysis if e["root_extraction"].get("root_type") == "mabni"),
    }
    
    pattern_stats = {
        "patterns_identified": sum(1 for e in enhanced_analysis if e["cv_pattern"]["pattern_type"]),
        "pattern_types": {},
    }
    
    for e in enhanced_analysis:
        if e["cv_pattern"]["pattern_type"]:
            pt = e["cv_pattern"]["pattern_type"]
            pattern_stats["pattern_types"][pt] = pattern_stats["pattern_types"].get(pt, 0) + 1
    
    stats = {
        "total_words": total_words,
        "morphology": {
            "words_with_case": morph_with_case,
            "case_coverage": morph_with_case / total_words if total_words > 0 else 0.0,
        },
        "syntax": {
            "predictions": syntax_predictions,
            "coverage": syntax_predictions / total_words if total_words > 0 else 0.0,
            "average_confidence": avg_confidence,
            "i3rab_distribution": i3rab_counts,
            "role_distribution": role_counts,
        },
        "operators": operator_stats,
        "roots": root_stats,
        "cv_patterns": pattern_stats,
    }
    
    if verbose:
        print(f"Total words: {total_words}", file=sys.stderr)
        print(f"Operators: {operator_stats['total_operators']} ({operator_stats['total_operators']/total_words*100:.1f}%)", file=sys.stderr)
        print(f"Roots: {root_stats['total_roots_extracted']} ({root_stats['total_roots_extracted']/total_words*100:.1f}%)", file=sys.stderr)
        print(f"CV Patterns: {pattern_stats['patterns_identified']} ({pattern_stats['patterns_identified']/total_words*100:.1f}%)", file=sys.stderr)
    
    return stats


def process_complete_pipeline(text: str, args: argparse.Namespace) -> Dict[str, Any]:
    """Process text through complete pipeline."""
    verbose = args.verbose
    
    if verbose:
        print("=" * 80, file=sys.stderr)
        print("COMPLETE PIPELINE SNAPSHOT - Sprints 1-4 + Enhanced Features", file=sys.stderr)
        print("=" * 80, file=sys.stderr)
    
    # Load operators catalog
    operators_catalog = load_operators_catalog(verbose)
    mabniyat_catalog = load_mabniyat_catalog(verbose)
    
    # Sprint 4 Component Demonstrations
    parser_demo = demo_i3rab_parser(verbose)
    evaluator_demo = demo_syntax_evaluator(verbose)
    bridge_demo = demo_morph_syntax_bridge(verbose)
    
    # Full Pipeline
    orthography_result = process_orthography(text, verbose)
    
    words = orthography_result["no_diacritics"].split()
    
    if verbose:
        print(f"\nTokenized into {len(words)} words", file=sys.stderr)
    
    # Enhanced analysis
    enhanced_analysis = enhance_word_analysis(words, operators_catalog, mabniyat_catalog, verbose)
    
    # Morphology
    morphology_results = extract_morphology(words, verbose)
    
    # Syntax
    syntax_results = predict_syntax(morphology_results, verbose)
    
    # Combine
    for syntax_res, enhanced in zip(syntax_results, enhanced_analysis):
        syntax_res["operator_analysis"] = enhanced["operator_analysis"]
        syntax_res["cv_pattern"] = enhanced["cv_pattern"]
        syntax_res["root_extraction"] = enhanced["root_extraction"]
    
    # Statistics
    statistics = compute_statistics(syntax_results, enhanced_analysis, verbose)
    
    result = {
        "metadata": {
            "title": "Complete Pipeline Snapshot - Sprints 1-4 + Enhanced Features",
            "source": "Ø¢ÙŠØ© Ø§Ù„Ø¯ÙŠÙ† (Al-Baqarah 2:282)",
            "date": "2026-02-21",
            "total_tests": 564,
            "pipeline_version": "2.0.0",
            "features": ["orthography", "morphology", "syntax", "operators", "cv_patterns", "roots", "mabniyat"],
            "operators_catalog_loaded": len(operators_catalog),
            "mabniyat_catalog_loaded": len(mabniyat_catalog),
            "sprints": [
                {"number": 1, "name": "Orthography", "status": "complete"},
                {"number": 2, "name": "Evaluation", "status": "complete"},
                {"number": 3, "name": "Morphology", "status": "complete"},
                {"number": 4, "name": "Syntax", "status": "complete"},
            ]
        },
        
        "sprint4_demonstrations": {
            "task_4_2_parser": parser_demo,
            "task_4_3_evaluator": evaluator_demo,
            "task_4_4_bridge": bridge_demo,
        },
        
        "full_pipeline": {
            "sprint1_orthography": orthography_result,
            "sprint3_morphology": {
                "words_analyzed": len(morphology_results),
                "sample_words": morphology_results[:10],
            },
            "sprint4_syntax": {
                "words_analyzed": len(syntax_results),
                "sample_words": syntax_results[:10],
            },
            "enhanced_features": {
                "operators": {
                    "total_detected": statistics["operators"]["total_operators"],
                    "distribution": statistics["operators"]["operator_groups"],
                    "sample_analysis": [e["operator_analysis"] for e in enhanced_analysis[:10]],
                },
                "cv_patterns": {
                    "total_identified": statistics["cv_patterns"]["patterns_identified"],
                    "pattern_types": statistics["cv_patterns"]["pattern_types"],
                    "sample_analysis": [e["cv_pattern"] for e in enhanced_analysis[:10]],
                },
                "roots": {
                    "total_extracted": statistics["roots"]["total_roots_extracted"],
                    "trilateral": statistics["roots"]["trilateral_roots"],
                    "quadrilateral": statistics["roots"]["quadrilateral_roots"],
                    "sample_analysis": [e["root_extraction"] for e in enhanced_analysis[:10]],
                },
            },
            "sprint2_statistics": statistics,
        },
        
        "complete_word_analysis": syntax_results,
    }
    
    return result


def main() -> int:
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("snapshot_out.json"),
        help="Output JSON file (default: snapshot_out.json)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print verbose output to stderr",
    )
    parser.add_argument(
        "--phonology-v2",
        action="store_true",
        help="Use Phonology V2 engine (if available)",
    )
    parser.add_argument(
        "--text",
        type=str,
        default=AYAT_AL_DAYN,
        help="Custom text to process (default: Ayat al-Dayn)",
    )
    args = parser.parse_args()

    try:
        result = process_complete_pipeline(args.text, args)
        
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        if args.verbose:
            print("\n" + "=" * 80, file=sys.stderr)
            print("âœ… PIPELINE COMPLETE", file=sys.stderr)
            print("=" * 80, file=sys.stderr)
            
            stats = result["full_pipeline"]["sprint2_statistics"]
            print(f"\nğŸ“Š Summary:", file=sys.stderr)
            print(f"  Total words: {stats['total_words']}", file=sys.stderr)
            print(f"  Operators: {stats['operators']['total_operators']} ({stats['operators']['total_operators']/stats['total_words']*100:.1f}%)", file=sys.stderr)
            print(f"  Roots: {stats['roots']['total_roots_extracted']} ({stats['roots']['total_roots_extracted']/stats['total_words']*100:.1f}%)", file=sys.stderr)
            print(f"  Mabniyat: {stats['roots']['mabniyat_identified']} ({stats['roots']['mabniyat_identified']/stats['total_words']*100:.1f}%)", file=sys.stderr)
            print(f"  CV Patterns: {stats['cv_patterns']['patterns_identified']} ({stats['cv_patterns']['patterns_identified']/stats['total_words']*100:.1f}%)", file=sys.stderr)
            print(f"  Syntax coverage: {stats['syntax']['coverage']:.1%}", file=sys.stderr)
            
            print("\n=" * 80, file=sys.stderr)
        
        print(f"Wrote {args.output}", file=sys.stderr)
        return 0
        
    except Exception as e:
        print(f"âŒ Error: {e}", file=sys.stderr)
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())