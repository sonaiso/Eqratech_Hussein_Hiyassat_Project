# Transformer fine-tuning configuration for the Quran dataset (Colab workflow)
# Fill in the dataset paths/checkpoints before launching a real training job.

run_name: quran-transformer-colab
seed: 42
model:
  base_model_name: aubmindlab/bert-base-arabertv02
  tokenizer_name: aubmindlab/bert-base-arabertv02
  cache_dir: /content/cache/models

dataset:
  train_file: data/quran/train.jsonl
  validation_file: data/quran/validation.jsonl
  test_file: data/quran/test.jsonl
  text_column: text
  label_column: surah
  max_length: 256

training:
  output_dir: /content/outputs/quran-transformer
  num_train_epochs: 5
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  learning_rate: 3.0e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  gradient_accumulation_steps: 2
  fp16: true
  evaluation_strategy: steps
  eval_steps: 250
  logging_steps: 50
  save_steps: 500
  save_total_limit: 3
  metric_for_best_model: f1
  greater_is_better: true

inference:
  max_new_tokens: 64
  temperature: 0.7
