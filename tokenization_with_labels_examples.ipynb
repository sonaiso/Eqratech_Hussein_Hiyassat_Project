{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "tokenization_with_labels"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Tokenization with Labels for Supervised Learning\n",
        "\n",
        "This notebook demonstrates different methods to add labels to tokenized data.\n",
        "\n",
        "For detailed documentation, see **TOKENIZATION_LABELS_GUIDE.md**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "pip install transformers datasets tokenizers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import_utilities"
      },
      "outputs": [],
      "source": [
        "# Import the tokenization utility\n",
        "from tokenization_with_labels import TokenizationWithLabels, create_example_data_files\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_examples"
      },
      "outputs": [],
      "source": [
        "# Create example data files to demonstrate different methods\n",
        "create_example_data_files()\n",
        "print(\"Example data files created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method1_header"
      },
      "source": [
        "## Method 1: Labels in Separate File\n",
        "\n",
        "Use this when you have:\n",
        "- Text data in one file (e.g., quran_data.txt)\n",
        "- Labels in another file (e.g., labels.txt)\n",
        "- Both files have the same number of lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "method1_code"
      },
      "outputs": [],
      "source": [
        "# Method 1: Tokenize with labels from separate files\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "tokenized_data = tokenizer.tokenize_with_labels_from_separate_file(\n",
        "    'examples/quran_data_example.txt',\n",
        "    'examples/labels_example.txt'\n",
        ")\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_data)} items\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method2_header"
      },
      "source": [
        "## Method 2: Embedded Labels in CSV\n",
        "\n",
        "Use this when you have a CSV file with columns for both text and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "method2_code"
      },
      "outputs": [],
      "source": [
        "# Method 2: Tokenize with embedded labels from CSV\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "tokenized_data = tokenizer.tokenize_with_embedded_labels(\n",
        "    'examples/quran_with_labels_example.csv',\n",
        "    text_column='verse',\n",
        "    label_column='category'\n",
        ")\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_data)} items\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method3_header"
      },
      "source": [
        "## Method 3: Embedded Labels in JSON\n",
        "\n",
        "Use this when you have a JSON file with fields for both text and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "method3_code"
      },
      "outputs": [],
      "source": [
        "# Method 3: Tokenize with embedded labels from JSON\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "tokenized_data = tokenizer.tokenize_with_embedded_labels(\n",
        "    'examples/quran_with_labels_example.json',\n",
        "    text_column='verse',\n",
        "    label_column='category',\n",
        "    file_format='json'\n",
        ")\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_data)} items\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method4_header"
      },
      "source": [
        "## Method 4: Label Mapping\n",
        "\n",
        "Use this when you have a dictionary mapping line numbers to labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "method4_code"
      },
      "outputs": [],
      "source": [
        "# Method 4: Tokenize with label mapping\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "# Define label mapping (line index to label)\n",
        "label_mapping = {\n",
        "    0: 'basmala',\n",
        "    1: 'praise',\n",
        "    2: 'attribute'\n",
        "}\n",
        "\n",
        "tokenized_data = tokenizer.tokenize_with_label_mapping(\n",
        "    'examples/quran_data_example.txt',\n",
        "    label_mapping\n",
        ")\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_data)} items\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "method5_header"
      },
      "source": [
        "## Method 5: Pattern-Based Labels\n",
        "\n",
        "Use this when labels can be derived from the text itself using rules or patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "method5_code"
      },
      "outputs": [],
      "source": [
        "# Method 5: Tokenize with pattern-based labels\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "# Define a function to derive labels from text\n",
        "def get_label_from_text(text):\n",
        "    if 'بِسْمِ اللَّهِ' in text:\n",
        "        return 'basmala'\n",
        "    elif 'الْحَمْدُ' in text:\n",
        "        return 'praise'\n",
        "    elif len(text) > 50:\n",
        "        return 'long_verse'\n",
        "    else:\n",
        "        return 'short_verse'\n",
        "\n",
        "tokenized_data = tokenizer.tokenize_with_pattern_based_labels(\n",
        "    'examples/quran_data_example.txt',\n",
        "    get_label_from_text\n",
        ")\n",
        "\n",
        "print(f\"Tokenized {len(tokenized_data)} items\")\n",
        "print(\"\\nFirst example:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "transformers_header"
      },
      "source": [
        "## Using with Hugging Face Transformers\n",
        "\n",
        "You can integrate a pre-trained tokenizer from the transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efffdefa"
      },
      "outputs": [],
      "source": [
        "# CELL ID: efffdefa - Tokenization with labels using transformers\n",
        "\n",
        "# Import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from tokenization_with_labels import TokenizationWithLabels\n",
        "\n",
        "# Load a pre-trained Arabic tokenizer\n",
        "# Options: 'aubmindlab/bert-base-arabertv2', 'asafaya/bert-base-arabic', etc.\n",
        "hf_tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv2')\n",
        "\n",
        "# Create tokenizer with the Hugging Face tokenizer\n",
        "tokenizer = TokenizationWithLabels(tokenizer=hf_tokenizer)\n",
        "\n",
        "# Example: Use any method to add labels\n",
        "# Replace with your actual data files\n",
        "tokenized_data = tokenizer.tokenize_with_labels_from_separate_file(\n",
        "    'examples/quran_data_example.txt',  # Your quran_data file\n",
        "    'examples/labels_example.txt'       # Your labels file\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(f\"Tokenized {len(tokenized_data)} items with labels\")\n",
        "print(\"\\nFirst example with transformer tokens:\")\n",
        "print(json.dumps(tokenized_data[0], ensure_ascii=False, indent=2))\n",
        "\n",
        "# Save the tokenized data\n",
        "tokenizer.save_tokenized_data(\n",
        "    tokenized_data,\n",
        "    'tokenized_quran_with_labels.json',\n",
        "    format='json'\n",
        ")\n",
        "\n",
        "print(\"\\nTokenized data saved to tokenized_quran_with_labels.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_header"
      },
      "source": [
        "## Saving Tokenized Data\n",
        "\n",
        "Save your tokenized data with labels for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_code"
      },
      "outputs": [],
      "source": [
        "# Save tokenized data with labels\n",
        "tokenizer = TokenizationWithLabels()\n",
        "\n",
        "# Assuming you have tokenized_data from one of the methods above\n",
        "tokenizer.save_tokenized_data(\n",
        "    tokenized_data,\n",
        "    'output_tokenized_data.json',\n",
        "    format='json'\n",
        ")\n",
        "\n",
        "# Or save as CSV\n",
        "tokenizer.save_tokenized_data(\n",
        "    tokenized_data,\n",
        "    'output_tokenized_data.csv',\n",
        "    format='csv'\n",
        ")\n",
        "\n",
        "print(\"Data saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Identify your label source**: Determine where your labels are or how they should be created\n",
        "2. **Prepare your data**: Organize your quran_data and labels according to one of the methods above\n",
        "3. **Choose the appropriate method**: Select the method that best fits your data organization\n",
        "4. **Tokenize with labels**: Run the tokenization code with your actual data files\n",
        "5. **Verify the results**: Check that labels are correctly associated with each text\n",
        "6. **Train your model**: Use the tokenized data with labels for supervised learning\n",
        "\n",
        "For more details, refer to **TOKENIZATION_LABELS_GUIDE.md**"
      ]
    }
  ]
}