{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN2m79tqJXuUc9sndozXK9G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salemqundil/Eqratech_Arabic_Diana_Project/blob/main/connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZqRB_NlKw2J"
      },
      "outputs": [],
      "source": [
        "    pip install colabcode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch\n"
      ],
      "metadata": {
        "id": "bLkzRji-LD7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade transformers accelerate"
      ],
      "metadata": {
        "id": "uJXGelVfLMtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  --upgrade transformers accelerate datasets tokenizers sentencepiece -quit"
      ],
      "metadata": {
        "id": "RScnezOcLbsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7ZSE4ErMMCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Colab-ready: PyTorch + CUDA check\n",
        "import platform\n",
        "import torch\n",
        "\n",
        "print(f\"Python:  {platform.python_version()}\")\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # List GPUs (will work in Colab GPU runtimes)\n",
        "    try:\n",
        "        import subprocess\n",
        "        print(subprocess.check_output([\"nvidia-smi\", \"-L\"], text=True).strip())\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    # quick smoke test on GPU\n",
        "    x = torch.randn(3, 3, device=device)\n",
        "    print(\"Tensor device:\", x.device)\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    x = torch.randn(3, 3, device=device)\n",
        "    print(\"Tensor device:\", x.device)\n"
      ],
      "metadata": {
        "id": "zTMAK8MLNEWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "training_loop_cell"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Training Loop Implementation\n",
        "# This cell demonstrates proper training loop with label handling\n",
        "\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Example: Assuming you have already set up:\n",
        "# - model: Your PyTorch/Transformers model\n",
        "# - dataloader: DataLoader with batches containing input_ids, attention_mask, token_type_ids, and labels\n",
        "# - optimizer: Your optimizer (e.g., AdamW)\n",
        "# - EPOCHS: Number of training epochs\n",
        "\n",
        "# Check for GPU and set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)  # Move model to the selected device\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set model to training mode\n",
        "model.train()\n",
        "\n",
        "print(\"Starting training loop...\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\nEpoch {epoch + 1}/{EPOCHS}\")\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Validate dataloader is not empty\n",
        "    if len(dataloader) == 0:\n",
        "        print(f\"Warning: Dataloader is empty. Skipping epoch {epoch + 1}\")\n",
        "        continue\n",
        "    \n",
        "    # Iterate over batches in the dataloader\n",
        "    for batch in tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\"):\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        token_type_ids = batch['token_type_ids'].to(device)\n",
        "        \n",
        "        # IMPORTANT: Ensure your dataset returns 'labels' in __getitem__\n",
        "        if 'labels' not in batch:\n",
        "            raise ValueError(\n",
        "                \"Labels are required for training but not found in batch. \"\n",
        "                \"Please update your dataset's __getitem__ method to include 'labels'.\"\n",
        "            )\n",
        "        \n",
        "        labels = batch['labels'].to(device)\n",
        "        \n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        # For models like BertForSequenceClassification, passing labels will\n",
        "        # automatically calculate the loss\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            labels=labels\n",
        "        )\n",
        "        \n",
        "        # Get the loss (automatically computed by the model)\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    # Print average loss for the epoch\n",
        "    if num_batches > 0:\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss:.4f}\")\n",
        "    else:\n",
        "        print(f\"Epoch {epoch + 1} finished. No batches were processed.\")\n",
        "\n",
        "print(\"\nTraining finished.\")\n",
        "\n",
        "# Save the model\n",
        "# torch.save(model.state_dict(), 'model.pth')\n",
        "# print(\"Model saved to model.pth\")\n"
      ]
    }
  ]
}