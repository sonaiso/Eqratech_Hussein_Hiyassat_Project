'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\nComplete Pipeline Snapshot - Integrated Version\nCombines operators, CV patterns, roots, mabniyat, and wazn matching.\n"""\n\nimport argparse\nimport json\nimport sys\nimport csv\nimport unicodedata\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass\nfrom collections import Counter\n\n# Arabic diacritics constants\nFATHATAN = "\u064B"\nDAMMATAN = "\u064C"\nKASRATAN = "\u064D"\nFATHA = "\u064E"\nDAMMA = "\u064F"\nKASRA = "\u0650"\nSHADDA = "\u0651"\nSUKUN = "\u0652"\nDAGGER_ALIF = "\u0670"\nALIF_WASLA = "\u0671"\n\nTANWIN = {FATHATAN, DAMMATAN, KASRATAN}\nVOWELS = {FATHA, DAMMA, KASRA, SUKUN}\nDIACRITICS = set().union(TANWIN, VOWELS, {SHADDA, DAGGER_ALIF})\nPLACEHOLDERS = {"Ù", "Ø¹", "Ù„"}\n\n# Configuration\nREQUIRE_FAL_ORDER_IN_PATTERN = True\nMIN_PATTERN_UNITS = 3\nSUBSTRING_MATCHING = True\nALLOW_MISSING_WORD_VOWELS = True\nIGNORE_LAST_VOWEL = False\nIGNORE_TANWIN = False\n\n# Test text (Ayat al-Dayn)\nAYAT_AL_DAYN = (\n    "ÙŠÙØ§ Ø£ÙÙŠÙÙ‘Ù‡ÙØ§ Ø§Ù„ÙÙ‘Ø°ÙÙŠÙ†Ù Ø¢Ù…ÙÙ†ÙÙˆØ§ Ø¥ÙØ°ÙØ§ ØªÙØ¯ÙØ§ÙŠÙÙ†ØªÙÙ… Ø¨ÙØ¯ÙÙŠÙ’Ù†Ù Ø¥ÙÙ„ÙÙ‰ Ø£ÙØ¬ÙÙ„Ù Ù…ÙÙ‘Ø³ÙÙ…Ù‹Ù‘Ù‰ ÙÙØ§ÙƒÙ’ØªÙØ¨ÙÙˆÙ‡Ù ÙˆÙÙ„Ù’ÙŠÙÙƒÙ’ØªÙØ¨ Ø¨ÙÙ‘ÙŠÙ’Ù†ÙÙƒÙÙ…Ù’ ÙƒÙØ§ØªÙØ¨ÙŒ Ø¨ÙØ§Ù„Ù’Ø¹ÙØ¯Ù’Ù„Ù ÙˆÙÙ„ÙØ§ ÙŠÙØ£Ù’Ø¨Ù ÙƒÙØ§ØªÙØ¨ÙŒ Ø£ÙÙ† ÙŠÙÙƒÙ’ØªÙØ¨Ù ÙƒÙÙ…ÙØ§ Ø¹ÙÙ„ÙÙ‘Ù…ÙÙ‡Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙÙÙ„Ù’ÙŠÙÙƒÙ’ØªÙØ¨Ù’ ÙˆÙÙ„Ù’ÙŠÙÙ…Ù’Ù„ÙÙ„Ù Ø§Ù„ÙÙ‘Ø°ÙÙŠ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù Ø§Ù„Ù’Ø­ÙÙ‚ÙÙ‘ ÙˆÙÙ„Ù’ÙŠÙØªÙÙ‘Ù‚Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘Ù‡Ù ÙˆÙÙ„ÙØ§ ÙŠÙØ¨Ù’Ø®ÙØ³Ù’ Ù…ÙÙ†Ù’Ù‡Ù Ø´ÙÙŠÙ’Ø¦Ø§Ù‹ ÙÙØ¥ÙÙ† ÙƒÙØ§Ù†Ù Ø§Ù„ÙÙ‘Ø°ÙÙŠ Ø¹ÙÙ„ÙÙŠÙ’Ù‡Ù Ø§Ù„Ù’Ø­ÙÙ‚ÙÙ‘ Ø³ÙÙÙÙŠÙ‡Ø§Ù‹ Ø£ÙÙˆÙ’ Ø¶ÙØ¹ÙÙŠÙØ§Ù‹ Ø£ÙÙˆÙ’ Ù„ÙØ§ ÙŠÙØ³Ù’ØªÙØ·ÙÙŠØ¹Ù Ø£ÙÙ† ÙŠÙÙ…ÙÙ„ÙÙ‘ Ù‡ÙÙˆÙ ÙÙÙ„Ù’ÙŠÙÙ…Ù’Ù„ÙÙ„Ù’ ÙˆÙÙ„ÙÙŠÙÙ‘Ù‡Ù Ø¨ÙØ§Ù„Ù’Ø¹ÙØ¯Ù’Ù„Ù ÙˆÙØ§Ø³Ù’ØªÙØ´Ù’Ù‡ÙØ¯ÙÙˆØ§ Ø´ÙÙ‡ÙÙŠØ¯ÙÙŠÙ’Ù†Ù Ù…ÙÙ† Ø±ÙÙ‘Ø¬ÙØ§Ù„ÙÙƒÙÙ…Ù’ ÙÙØ¥ÙÙ† Ù„ÙÙ‘Ù…Ù’ ÙŠÙÙƒÙÙˆÙ†ÙØ§ Ø±ÙØ¬ÙÙ„ÙÙŠÙ’Ù†Ù ÙÙØ±ÙØ¬ÙÙ„ÙŒ ÙˆÙØ§Ù…Ù’Ø±ÙØ£ÙØªÙØ§Ù†Ù Ù…ÙÙ…ÙÙ‘Ù† ØªÙØ±Ù’Ø¶ÙÙˆÙ’Ù†Ù Ù…ÙÙ†Ù Ø§Ù„Ø´ÙÙ‘Ù‡ÙØ¯ÙØ§Ø¡Ù Ø£ÙÙ† ØªÙØ¶ÙÙ„ÙÙ‘ Ø¥ÙØ­Ù’Ø¯ÙØ§Ù‡ÙÙ…ÙØ§ ÙÙØªÙØ°ÙÙƒÙÙ‘Ø±Ù Ø¥ÙØ­Ù’Ø¯ÙØ§Ù‡ÙÙ…ÙØ§ Ø§Ù„Ù’Ø£ÙØ®Ù’Ø±ÙÙ‰ ÙˆÙÙ„ÙØ§ ÙŠÙØ£Ù’Ø¨Ù Ø§Ù„Ø´ÙÙ‘Ù‡ÙØ¯ÙØ§Ø¡Ù Ø¥ÙØ°ÙØ§ Ù…ÙØ§ Ø¯ÙØ¹ÙÙˆØ§ ÙˆÙÙ„ÙØ§ ØªÙØ³Ù’Ø£ÙÙ…ÙÙˆØ§ Ø£ÙÙ† ØªÙÙƒÙ’ØªÙØ¨ÙÙˆÙ‡Ù ØµÙØºÙÙŠØ±Ø§Ù‹ Ø£ÙÙˆÙ’ ÙƒÙØ¨ÙÙŠØ±Ø§Ù‹ Ø¥ÙÙ„ÙÙ‰ Ø£ÙØ¬ÙÙ„ÙÙ‡Ù Ø°ÙÙ„ÙÙƒÙÙ…Ù’ Ø£ÙÙ‚Ù’Ø³ÙØ·Ù Ø¹ÙÙ†Ø¯Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙØ£ÙÙ‚Ù’ÙˆÙÙ…Ù Ù„ÙÙ„Ø´ÙÙ‘Ù‡ÙØ§Ø¯ÙØ©Ù ÙˆÙØ£ÙØ¯Ù’Ù†ÙÙ‰ Ø£ÙÙ„ÙÙ‘Ø§ ØªÙØ±Ù’ØªÙØ§Ø¨ÙÙˆØ§ Ø¥ÙÙ„ÙÙ‘Ø§ Ø£ÙÙ† ØªÙÙƒÙÙˆÙ†Ù ØªÙØ¬ÙØ§Ø±ÙØ©Ù‹ Ø­ÙØ§Ø¶ÙØ±ÙØ©Ù‹ ØªÙØ¯ÙÙŠØ±ÙÙˆÙ†ÙÙ‡ÙØ§ Ø¨ÙÙŠÙ’Ù†ÙÙƒÙÙ…Ù’ ÙÙÙ„ÙÙŠÙ’Ø³Ù Ø¹ÙÙ„ÙÙŠÙ’ÙƒÙÙ…Ù’ Ø¬ÙÙ†ÙØ§Ø­ÙŒ Ø£ÙÙ„ÙÙ‘Ø§ ØªÙÙƒÙ’ØªÙØ¨ÙÙˆÙ‡ÙØ§ ÙˆÙØ£ÙØ´Ù’Ù‡ÙØ¯ÙÙˆØ§ Ø¥ÙØ°ÙØ§ ØªÙØ¨ÙØ§ÙŠÙØ¹Ù’ØªÙÙ…Ù’ ÙˆÙÙ„ÙØ§ ÙŠÙØ¶ÙØ§Ø±ÙÙ‘ ÙƒÙØ§ØªÙØ¨ÙŒ ÙˆÙÙ„ÙØ§ Ø´ÙÙ‡ÙÙŠØ¯ÙŒ ÙˆÙØ¥ÙÙ† ØªÙÙÙ’Ø¹ÙÙ„ÙÙˆØ§ ÙÙØ¥ÙÙ†ÙÙ‘Ù‡Ù ÙÙØ³ÙÙˆÙ‚ÙŒ Ø¨ÙÙƒÙÙ…Ù’ ÙˆÙØ§ØªÙÙ‘Ù‚ÙÙˆØ§ Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙÙŠÙØ¹ÙÙ„ÙÙ‘Ù…ÙÙƒÙÙ…Ù Ø§Ù„Ù„ÙÙ‘Ù‡Ù ÙˆÙØ§Ù„Ù„ÙÙ‘Ù‡Ù Ø¨ÙÙƒÙÙ„ÙÙ‘ Ø´ÙÙŠÙ’Ø¡Ù Ø¹ÙÙ„ÙÙŠÙ…ÙŒ"\n)\n\n\n@dataclass(frozen=True)\nclass Unit:\n    """Represents a base letter + diacritics unit."""\n    base: str\n    diacs: Tuple[str, ...]\n\n\n@dataclass\nclass MatchHit:\n    """Represents a wazn pattern match."""\n    pattern: str\n    reason: str  # FULLMATCH or WINDOW\n    window_start: int\n    score_key: Tuple[int, int, int, int]\n\n\ndef _sorted_tuple(s):\n    """Convert set to sorted tuple for deterministic comparison."""\n    return tuple(sorted(s))\n\ndef remove_al_and_shadda(word: str) -> str:\n    """Remove 'Ø§Ù„' definiteness and following shadda from word."""\n    if word.startswith('Ø§Ù„'):\n        remaining = word[2:]\n        chars = list(remaining)\n        if len(chars) >= 2:\n            for i in range(1, min(3, len(chars))):\n                if chars[i] == SHADDA:\n                    new_chars = chars[:i] + chars[i+1:]\n                    remaining = ''.join(new_chars)\n                    break\n        return remaining\n    return word\n\n\ndef split_units(text: str) -> List[Unit]:\n    """Split Arabic text into (base_letter + diacritics) units."""\n    units: List[Unit] = []\n    cur_base: Optional[str] = None\n    cur_diacs = []\n\n    for ch in text:\n        if ch in DIACRITICS:\n            if cur_base is None:\n                continue\n            cur_diacs.append(ch)\n        else:\n            if cur_base is not None:\n                units.append(Unit(cur_base, _sorted_tuple(cur_diacs)))\n            cur_base = ch\n            cur_diacs = []\n\n    if cur_base is not None:\n        units.append(Unit(cur_base, _sorted_tuple(cur_diacs)))\n\n    return units\n\ndef expand_shadda(units: List[Unit]) -> List[Unit]:\n    """Expand shadda into two consonants."""\n    expanded = []\n    for unit in units:\n        if SHADDA in unit.diacs:\n            second_marks = [m for m in unit.diacs if m != SHADDA]\n            expanded.append(Unit(unit.base, (SUKUN,)))\n            expanded.append(Unit(unit.base, _sorted_tuple(second_marks)))\n        else:\n            expanded.append(unit)\n    return expanded\n\ndef has_fal_order_in_pattern(pattern: str) -> bool:\n    """Check if pattern contains Ù then Ø¹ then Ù„ in order."""\n    bases = [u.base for u in split_units(pattern)]\n    try:\n        i_f = bases.index("Ù")\n        i_a = bases.index("Ø¹", i_f + 1)\n        i_l = bases.index("Ù„", i_a + 1)\n        return True\n    except ValueError:\n        return False\n\ndef pattern_effective_len(units: List[Unit]) -> int:\n    """Calculate effective length including shadda complexity."""\n    shadda_count = sum(1 for u in units if SHADDA in u.diacs)\n    return len(units) + shadda_count\n\ndef count_fixed_letters(units: List[Unit]) -> int:\n    """Count non-placeholder letters."""\n    return sum(1 for u in units if u.base not in PLACEHOLDERS)\n\ndef count_specified_diacritics(units: List[Unit]) -> int:\n    """Count specified vowels, tanwin, and shadda."""\n    c = 0\n    for u in units:\n        for d in u.diacs:\n            if d in VOWELS or d in TANWIN or d == SHADDA:\n                c += 1\n    return c\n\ndef unit_vowel(diacs: Tuple[str, ...]) -> Optional[str]:\n    """Extract vowel from diacritics."""\n    for d in diacs:\n        if d in VOWELS:\n            return d\n    return None\n\ndef unit_tanwin(diacs: Tuple[str, ...]) -> Optional[str]:\n    """Extract tanwin from diacritics."""\n    for d in diacs:\n        if d in TANWIN:\n            return d\n    return None\n\ndef unit_has_shadda(diacs: Tuple[str, ...]) -> bool:\n    """Check if unit has shadda."""\n    return SHADDA in diacs\n\ndef normalize_units_for_options(units: List[Unit], ignore_last_vowel: bool, ignore_tanwin: bool) -> List[Unit]:\n    """Normalize units according to options."""\n    if not units:\n        return units\n    out = []\n    for idx, u in enumerate(units):\n        diacs = set(u.diacs)\n        if ignore_tanwin:\n            diacs -= TANWIN\n        if ignore_last_vowel and idx == len(units) - 1:\n            diacs -= VOWELS\n            diacs -= TANWIN\n        out.append(Unit(u.base, _sorted_tuple(diacs)))\n    return out\n\ndef units_match(p: Unit, w: Unit, allow_missing_word_vowels: bool) -> bool:\n    """Check if pattern unit matches word unit."""\n    # Base letter matching\n    if p.base in PLACEHOLDERS:\n        base_ok = True\n    else:\n        base_ok = (p.base == w.base)\n    if not base_ok:\n        return False\n\n    # Shadda matching\n    p_sh = unit_has_shadda(p.diacs)\n    w_sh = unit_has_shadda(w.diacs)\n    \n    if p_sh and not w_sh:\n        return False\n    elif not p_sh and w_sh and p.base not in PLACEHOLDERS:\n        return False\n\n    # Vowel matching\n    pv = unit_vowel(p.diacs)\n    wv = unit_vowel(w.diacs)\n    if pv is not None:\n        if wv is None and allow_missing_word_vowels:\n            pass\n        else:\n            if pv != wv:\n                return False\n\n    # Tanwin matching\n    pt = unit_tanwin(p.diacs)\n    wt = unit_tanwin(w.diacs)\n    if pt is not None:\n        if wt is None and allow_missing_word_vowels:\n            pass\n        else:\n            if pt != wt:\n                return False\n\n    return True\n\ndef try_match_pattern_to_word(pattern: str, word: str) -> List[MatchHit]:\n    """Try to match a wazn pattern to a word."""\n    word_processed = remove_al_and_shadda(word)\n    \n    p_units = split_units(pattern)\n    w_units = split_units(word_processed)\n\n    p_units = normalize_units_for_options(p_units, IGNORE_LAST_VOWEL, IGNORE_TANWIN)\n    w_units = normalize_units_for_options(w_units, IGNORE_LAST_VOWEL, IGNORE_TANWIN)\n\n    if len(p_units) < MIN_PATTERN_UNITS:\n        return []\n    if REQUIRE_FAL_ORDER_IN_PATTERN and not has_fal_order_in_pattern(pattern):\n        return []\n\n    lp = len(p_units)\n    lw = len(w_units)\n\n    fixed = count_fixed_letters(p_units)\n    diac_spec = count_specified_diacritics(p_units)\n    eff_len = pattern_effective_len(p_units)\n\n    def make_score(reason: str) -> Tuple[int, int, int, int]:\n        reason_rank = 10 if reason == "FULLMATCH" else 1\n        return (reason_rank, eff_len, fixed, diac_spec)\n\n    if lp > lw:\n        return []\n\n    # Full match\n    if lp == lw:\n        ok = True\n        for i in range(lp):\n            if not units_match(p_units[i], w_units[i], ALLOW_MISSING_WORD_VOWELS):\n                ok = False\n                break\n        if ok:\n            return [MatchHit(pattern, "FULLMATCH", 0, make_score("FULLMATCH"))]\n        return []\n\n    # Window matching\n    if not SUBSTRING_MATCHING:\n        return []\n\n    best_start = None\n    for start in range(0, lw - lp + 1):\n        ok = True\n        for i in range(lp):\n            if not units_match(p_units[i], w_units[start + i], ALLOW_MISSING_WORD_VOWELS):\n                ok = False\n                break\n        if ok:\n            best_start = start\n            break\n\n    if best_start is None:\n        return []\n\n    return [MatchHit(pattern, "WINDOW", best_start, make_score("WINDOW"))]\n\ndef best_hit(hits: List[MatchHit]) -> Optional[MatchHit]:\n    """Select best match from hits."""\n    if not hits:\n        return None\n    hits_sorted = sorted(\n        hits,\n        key=lambda h: (h.score_key, len(h.pattern), h.pattern),\n        reverse=True\n    )\n    return hits_sorted[0]\n\ndef detect_cv_pattern_integrated(word: str) -> Dict[str, Any]:\n    """Detect CV pattern using integrated unit-based approach."""\n    units = split_units(word)\n    units = expand_shadda(units)\n    \n    pattern = []\n    i = 0\n    \n    # Handle initial hamza\n    if units and units[0].base in {ALIF_WASLA, "Ø£", "Ø¥", "Ø¢"}:\n        pattern.extend(["C", "V"])\n        units = units[1:]\n    \n    # Process remaining units\n    prev_marks = []\n    for unit in units:\n        if not unit.base.isalpha():\n            prev_marks = unit.diacs\n            continue\n        \n        # Check for madd\n        is_madd = False\n        if unit.base == "Ø§":\n            is_madd = any(m in {FATHA, FATHATAN} for m in prev_marks)\n        elif unit.base == "Ùˆ":\n            is_madd = any(m in {DAMMA, DAMMATAN} for m in prev_marks)\n        elif unit.base in {"ÙŠ", "Ù‰"}:\n            is_madd = any(m in {KASRA, KASRATAN} for m in prev_marks)\n        elif unit.base == "Ø¢":\n            pattern.append("C")\n            is_madd = False\n        \n        if is_madd:\n            pattern.append("V")\n        else:\n            pattern.append("C")\n            if any(m in VOWELS or m in TANWIN for m in unit.diacs):\n                pattern.append("V")\n        \n        prev_marks = unit.diacs\n    \n    pattern_str = ''.join(pattern)\n    \n    # Classify pattern type\n    pattern_type = None\n    if pattern_str == 'CVCVC':\n        pattern_type = 'faÊ•al (ÙÙØ¹ÙÙ„)'\n    elif pattern_str == 'CVCCVC':\n        pattern_type = 'faÊ•Ê•al (ÙÙØ¹ÙÙ‘Ù„)'\n    elif pattern_str == 'CVCVVC':\n        pattern_type = 'faÊ•aal (ÙÙØ¹ÙØ§Ù„)'\n    elif pattern_str == 'CVCVVCVC':\n        pattern_type = 'faÊ•aalah (ÙÙØ¹ÙØ§Ù„ÙØ©)'\n    elif pattern_str == 'CVCCVVC':\n        pattern_type = 'mafÊ•uul (Ù…ÙÙÙ’Ø¹ÙÙˆÙ„)'\n    elif pattern_str == 'CVCVCCVC':\n        pattern_type = 'mufaÊ•Ê•il (Ù…ÙÙÙØ¹ÙÙ‘Ù„)'\n    \n    # Check CV law\n    follows_cv_law = True\n    if len(pattern_str) < 2 or pattern_str[0] != "C" or pattern_str[1] != "V":\n        follows_cv_law = False\n    \n    return {\n        "pattern": pattern_str,\n        "pattern_type": pattern_type,\n        "length": len(pattern),\n        "consonant_count": pattern.count('C'),\n        "vowel_count": pattern.count('V'),\n        "follows_cv_law": follows_cv_law,\n    }\n\ndef load_operators_catalog(verbose: bool = False) -> Dict[str, Dict[str, Any]]:\n    """Load operators catalog from CSV file."""\n    catalog_path = Path("data/operators_catalog_split.csv")\n    \n    if not catalog_path.exists():\n        if verbose:\n            print(f"Warning: Operators catalog not found at {catalog_path}", file=sys.stderr)\n        return {}\n    \n    operators = {}\n    \n    try:\n        with open(catalog_path, 'r', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                operator = row['Operator'].strip()\n                operators[operator] = {\n                    "group_number": row['Group Number'],\n                    "arabic_group": row['Arabic Group Name'],\n                    "english_group": row['English Group Name'],\n                    "purpose": row['Purpose/Usage'],\n                    "example": row['Example'],\n                    "note": row['Note'],\n                }\n        \n        if verbose:\n            print(f"Loaded {len(operators)} operators from catalog", file=sys.stderr)\n        \n    except Exception as e:\n        if verbose:\n            print(f"Error loading operators catalog: {e}", file=sys.stderr)\n        return {}\n    \n    return operators\n\ndef load_mabniyat_catalog(verbose: bool = False) -> Dict[str, Dict[str, Any]]:\n    """Load Mabniyat (indeclinable nouns/particles) from data/arabic_json/2."""\n    catalog_path = Path("data/arabic_json/2")\n    mabniyat = {}\n    \n    if not catalog_path.exists():\n        if verbose:\n            print(f"Warning: Mabniyat catalog path not found at {catalog_path}", file=sys.stderr)\n        return {}\n    \n    count = 0\n    try:\n        for json_file in catalog_path.rglob("*.json"):\n            try:\n                with open(json_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    \n                if isinstance(data, list):\n                    items = data\n                else:\n                    items = [data]\n                    \n                for item in items:\n                    if not isinstance(item, dict):\n                        continue\n                        \n                    word = item.get("Ø§Ù„Ø£Ø¯Ø§Ø©")\n                    if not word:\n                        continue\n                    \n                    clean_word = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')\n                    forms = [f.strip() for f in clean_word.split('/')]\n                    \n                    for form in forms:\n                        if form:\n                            mabniyat[form] = item\n                            count += 1\n                            \n            except Exception as e:\n                if verbose:\n                    print(f"Error loading {json_file}: {e}", file=sys.stderr)\n                    \n        if verbose:\n            print(f"Loaded {count} Mabniyat entries", file=sys.stderr)\n            \n    except Exception as e:\n        if verbose:\n            print(f"Error walking mabniyat catalog: {e}", file=sys.stderr)\n            \n    return mabniyat\n\ndef sniff_delimiter(path: str) -> str:\n    """Detect CSV delimiter."""\n    with open(path, "r", encoding="utf-8", newline="") as f:\n        sample = f.read(4096)\n    try:\n        dialect = csv.Sniffer().sniff(sample, delimiters=[",", "\t", ";", "|"])\n        return dialect.delimiter\n    except Exception:\n        if "\t" in sample.splitlines()[0] if sample.splitlines() else "":\n            return "\t"\n        return ","\n\ndef load_wazn_patterns(verbose: bool = False) -> List[str]:\n    """Load wazn patterns from CSV."""\n    patterns_path = Path("data/awzan-claude-atwah.csv")\n    \n    if not patterns_path.exists():\n        if verbose:\n            print(f"Warning: Wazn patterns not found at {patterns_path}", file=sys.stderr)\n        return []\n    \n    patterns = []\n    try:\n        delim = sniff_delimiter(str(patterns_path))\n        with open(patterns_path, 'r', encoding='utf-8', newline='') as f:\n            reader = csv.DictReader(f, delimiter=delim)\n            for row in reader:\n                # Try different possible column names\n                pattern = row.get('Ø§Ù„ÙˆØ²Ù†') or row.get('wazn') or row.get('pattern') or row.get('Pattern')\n                if pattern and pattern.strip():\n                    patterns.append(pattern.strip())\n        \n        # Deduplicate\n        seen = set()\n        unique_patterns = []\n        for p in patterns:\n            if p not in seen:\n                seen.add(p)\n                unique_patterns.append(p)\n        \n        if verbose:\n            print(f"Loaded {len(unique_patterns)} wazn patterns", file=sys.stderr)\n        \n        return unique_patterns\n        \n    except Exception as e:\n        if verbose:\n            print(f"Error loading wazn patterns: {e}", file=sys.stderr)\n        return []\n\ndef extract_root(word: str, mabniyat_catalog: Optional[Dict[str, Dict[str, Any]]] = None) -> Dict[str, Any]:\n    """Extract tri-literal root from Arabic word."""\n    clean = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')\n    original_clean = clean\n    \n    # Check Mabniyat Catalog first\n    if mabniyat_catalog and clean in mabniyat_catalog:\n        mabniyat_info = mabniyat_catalog[clean]\n        return {\n            "original_word": word,\n            "cleaned": original_clean,\n            "stem": clean,\n            "root_trilateral": None,\n            "root_quadrilateral": None,\n            "root_type": "mabni",\n            "confidence": 1.0,\n            "consonants_extracted": 0,\n            "method": "knowledge_base_lookup",\n            "mabniyat_info": {\n                "type": mabniyat_info.get("Ø§Ù„Ù†ÙˆØ¹"),\n                "grammatical_case": mabniyat_info.get("Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø­ÙˆÙŠØ©"),\n                "number": mabniyat_info.get("Ø§Ù„Ø¹Ø¯Ø¯"),\n                "gender": mabniyat_info.get("Ø§Ù„Ø¬Ù†Ø³") or mabniyat_info.get("Ø§Ù„Ø¬Ù†Ø³ "),\n            }\n        }\n    \n    # Remove prefixes\n    prefixes = ["Ø§Ù„", "ÙˆØ§Ù„", "ÙØ§Ù„", "Ø¨Ø§Ù„", "ÙƒØ§Ù„", "Ù„Ù„", "Ùˆ", "Ù", "Ø¨", "Ù„", "Ùƒ", "Ø³", "Øª", "ÙŠ", "Ù†", "Ø£"]\n    for prefix in prefixes:\n        if clean.startswith(prefix) and len(clean) > len(prefix) + 2:\n            clean = clean[len(prefix):]\n            break\n    \n    # Remove suffixes\n    suffixes = ["ÙˆÙ†Ù‡", "ÙˆÙ‡Ø§", "Ù‡Ù…Ø§", "ÙƒÙ…Ø§", "ÙƒÙ†", "Ù‡Ù…", "Ù‡Ù†", "Ù†Ø§", "Ù†ÙŠ", "ÙˆØ§", "ÙˆÙ†", "ÙŠÙ†", "Ø§Ù†", "ØªØ§Ù†", "ØªÙŠÙ†", "Ø©", "Ù‡", "Ù‡Ø§", "Øª", "Ùƒ", "ÙŠ"]\n    for suffix in suffixes:\n        if clean.endswith(suffix) and len(clean) > len(suffix) + 2:\n            clean = clean[:-len(suffix)]\n            break\n    \n    # Extract consonantal root\n    consonants = []\n    weak_letters = set("Ø§ÙˆÙ‰ÙŠØ¡Ø¢Ø£Ø¥Ø¤Ø¦")\n    \n    for char in clean:\n        if char.isalpha() and char not in "Ù€":\n            if len(consonants) > 0 and char in weak_letters:\n                continue\n            consonants.append(char)\n    \n    if len(consonants) >= 3:\n        root_3 = ''.join(consonants[:3])\n        root_4 = ''.join(consonants[:4]) if len(consonants) >= 4 else None\n        confidence = 0.7 if len(consonants) == 3 else 0.6\n    elif len(consonants) == 2:\n        root_3 = ''.join(consonants)\n        root_4 = None\n        confidence = 0.3\n    else:\n        root_3 = None\n        root_4 = None\n        confidence = 0.0\n    \n    return {\n        "original_word": word,\n        "cleaned": original_clean,\n        "stem": clean,\n        "root_trilateral": root_3,\n        "root_quadrilateral": root_4,\n        "root_type": "trilateral" if root_3 and len(root_3) == 3 else "quadrilateral" if root_4 else "unknown",\n        "confidence": confidence,\n        "consonants_extracted": len(consonants),\n        "method": "morphological_stripping",\n    }\n\ndef detect_operator(word: str, operators_catalog: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:\n    """Detect Arabic operator (particle/verb) using catalog."""\n    clean_word = ''.join(c for c in word if c not in 'Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù°')\n    \n    # Direct match\n    if clean_word in operators_catalog:\n        return {\n            "is_operator": True,\n            "operator": clean_word,\n            "original_word": word,\n            **operators_catalog[clean_word],\n        }\n    \n    # Check for prefixed operators\n    prefixes = ["Ùˆ", "Ù", "Ø¨", "Ù„", "Ùƒ"]\n    for prefix in prefixes:\n        if clean_word.startswith(prefix) and len(clean_word) > 1:\n            stem = clean_word[1:]\n            if stem in operators_catalog:\n                return {\n                    "is_operator": False,\n                    "has_operator_prefix": True,\n                    "prefix": prefix,\n                    "prefix_operator": operators_catalog.get(prefix, {}),\n                    "stem": stem,\n                    "stem_operator": operators_catalog.get(stem, {}),\n                    "original_word": word,\n                }\n    \n    return {\n        "is_operator": False,\n        "has_operator_prefix": False,\n        "original_word": word,\n    }\n\ndef analyze_word(word: str, operators_catalog: Dict, mabniyat_catalog: Dict, wazn_patterns: List[str]) -> Dict[str, Any]:\n    """Perform complete analysis on a single word."""\n    # Operator analysis\n    operator_analysis = detect_operator(word, operators_catalog)\n    \n    # CV pattern\n    cv_pattern = detect_cv_pattern_integrated(word)\n    \n    # Root extraction\n    root_extraction = extract_root(word, mabniyat_catalog)\n    \n    # Wazn matching\n    wazn_matches = []\n    all_hits = []\n    for pattern in wazn_patterns:\n        hits = try_match_pattern_to_word(pattern, word)\n        if hits:\n            all_hits.extend(hits)\n    \n    if all_hits:\n        # Sort and take top matches\n        all_hits.sort(key=lambda h: (h.score_key, len(h.pattern), h.pattern), reverse=True)\n        for hit in all_hits[:3]:  # Top 3 matches\n            wazn_matches.append({\n                "pattern": hit.pattern,\n                "match_type": hit.reason,\n                "window_start": hit.window_start,\n                "score": list(hit.score_key),\n            })\n    \n    return {\n        "word": word,\n        "operator_analysis": operator_analysis,\n        "cv_pattern": cv_pattern,\n        "root_extraction": root_extraction,\n        "wazn_matches": wazn_matches,\n    }\n\ndef compute_statistics(word_analyses: List[Dict[str, Any]]) -> Dict[str, Any]:\n    """Compute statistics from word analyses."""\n    total_words = len(word_analyses)\n    \n    # Operator stats\n    operators_detected = sum(1 for w in word_analyses if w["operator_analysis"]["is_operator"])\n    \n    # Root stats\n    roots_extracted = sum(1 for w in word_analyses if w["root_extraction"]["root_trilateral"])\n    trilateral_roots = sum(1 for w in word_analyses if w["root_extraction"]["root_type"] == "trilateral")\n    quadrilateral_roots = sum(1 for w in word_analyses if w["root_extraction"]["root_type"] == "quadrilateral")\n    mabniyat = sum(1 for w in word_analyses if w["root_extraction"]["root_type"] == "mabni")\n    \n    # CV pattern stats\n    patterns_classified = sum(1 for w in word_analyses if w["cv_pattern"]["pattern_type"])\n    patterns_valid = sum(1 for w in word_analyses if w["cv_pattern"]["follows_cv_law"])\n    \n    # Wazn matching stats\n    total_matches = sum(len(w["wazn_matches"]) for w in word_analyses)\n    full_matches = sum(1 for w in word_analyses for m in w["wazn_matches"] if m["match_type"] == "FULLMATCH")\n    window_matches = sum(1 for w in word_analyses for m in w["wazn_matches"] if m["match_type"] == "WINDOW")\n    \n    return {\n        "total_words": total_words,\n        "operators": {\n            "total_detected": operators_detected,\n        },\n        "roots": {\n            "total_extracted": roots_extracted,\n            "trilateral": trilateral_roots,\n            "quadrilateral": quadrilateral_roots,\n            "mabniyat": mabniyat,\n        },\n        "cv_patterns": {\n            "classified": patterns_classified,\n            "valid": patterns_valid,\n        },\n        "wazn_matches": {\n            "total": total_matches,\n            "full_matches": full_matches,\n            "window_matches": window_matches,\n        }\n    }\n\ndef main() -> int:\n    parser = argparse.ArgumentParser(\n        description=__doc__,\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\n        "--output",\n        type=Path,\n        default=Path("snapshot_out.json"),\n        help="Output JSON file (default: snapshot_out.json)",\n    )\n    parser.add_argument(\n        "--verbose",\n        action="store_true",\n        help="Print verbose output to stderr",\n    )\n    parser.add_argument(\n        "--text",\n        type=str,\n        default=AYAT_AL_DAYN,\n        help="Custom text to process (default: Ayat al-Dayn)",\n    )\n    args = parser.parse_args()\n    \n    try:\n        if args.verbose:\n            print("=" * 80, file=sys.stderr)\n            print("COMPLETE PIPELINE - Enhanced Integration", file=sys.stderr)\n            print("=" * 80, file=sys.stderr)\n        \n        # Load catalogs\n        operators_catalog = load_operators_catalog(args.verbose)\n        mabniyat_catalog = load_mabniyat_catalog(args.verbose)\n        wazn_patterns = load_wazn_patterns(args.verbose)\n        \n        # Tokenize\n        words = args.text.split()\n        \n        if args.verbose:\n            print(f"\nTokenized into {len(words)} words", file=sys.stderr)\n        \n        # Analyze all words\n        word_analyses = []\n        for word in words:\n            analysis = analyze_word(word, operators_catalog, mabniyat_catalog, wazn_patterns)\n            word_analyses.append(analysis)\n        \n        # Compute statistics\n        statistics = compute_statistics(word_analyses)\n        \n        if args.verbose:\n            print(f"\n=== ENHANCED ANALYSIS ===", file=sys.stderr)\n            print(f"Operators: {statistics['operators']['total_detected']}/{statistics['total_words']} ({statistics['operators']['total_detected']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"Roots: {statistics['roots']['total_extracted']}/{statistics['total_words']} ({statistics['roots']['total_extracted']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"Mabniyat: {statistics['roots']['mabniyat']}/{statistics['total_words']} ({statistics['roots']['mabniyat']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"CV Patterns (classified): {statistics['cv_patterns']['classified']}/{statistics['total_words']} ({statistics['cv_patterns']['classified']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"CV Patterns (valid): {statistics['cv_patterns']['valid']}/{statistics['total_words']} ({statistics['cv_patterns']['valid']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"Wazn Matches: {statistics['wazn_matches']['total']}/{statistics['total_words']} ({statistics['wazn_matches']['total']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  - Full matches: {statistics['wazn_matches']['full_matches']}", file=sys.stderr)\n            print(f"  - Window matches: {statistics['wazn_matches']['window_matches']}", file=sys.stderr)\n        \n        # Build output\n        result = {\n            "metadata": {\n                "title": "Complete Pipeline - Enhanced Integration",\n                "source": "Ø¢ÙŠØ© Ø§Ù„Ø¯ÙŠÙ† (Al-Baqarah 2:282)",\n                "pipeline_version": "3.0.0",\n                "features": ["operators", "cv_patterns", "roots", "mabniyat", "wazn_matching"],\n                "catalogs_loaded": {\n                    "operators": len(operators_catalog),\n                    "mabniyat": len(set(mabniyat_catalog.keys())),\n                    "wazn_patterns": len(wazn_patterns),\n                }\n            },\n            "statistics": statistics,\n            "word_analysis": word_analyses,\n        }\n        \n        # Write output\n        with open(args.output, "w", encoding="utf-8") as f:\n            json.dump(result, f, ensure_ascii=False, indent=2)\n        \n        if args.verbose:\n            print("\n" + "=" * 80, file=sys.stderr)\n            print("âœ… PIPELINE COMPLETE", file=sys.stderr)\n            print("=" * 80, file=sys.stderr)\n            print(f"\nğŸ“Š Summary:", file=sys.stderr)\n            print(f"  Total words: {statistics['total_words']}", file=sys.stderr)\n            print(f"  Operators: {statistics['operators']['total_detected']} ({statistics['operators']['total_detected']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  Roots: {statistics['roots']['total_extracted']} ({statistics['roots']['total_extracted']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  Mabniyat: {statistics['roots']['mabniyat']} ({statistics['roots']['mabniyat']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  CV Patterns (classified): {statistics['cv_patterns']['classified']} ({statistics['cv_patterns']['classified']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  CV Patterns (valid): {statistics['cv_patterns']['valid']} ({statistics['cv_patterns']['valid']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print(f"  Wazn Matches: {statistics['wazn_matches']['total']} ({statistics['wazn_matches']['total']/statistics['total_words']*100:.1f}%)", file=sys.stderr)\n            print("\n" + "=" * 80, file=sys.stderr)\n        \n        print(f"Wrote {args.output}", file=sys.stderr)\n        return 0\n        \n    except Exception as e:\n        print(f"âŒ Error: {e}", file=sys.stderr)\n        if args.verbose:\n            import traceback\n            traceback.print_exc()\n        return 1\n\n\nif __name__ == "__main__":\n    sys.exit(main())\n'